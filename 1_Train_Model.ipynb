{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "looking-application",
   "metadata": {},
   "source": [
    "## Image Captioning - Training Notebook\n",
    "\n",
    "Make sure to look at setup the project environment and data according to the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alpha-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from pycocotools.coco import COCO\n",
    "from imcaption.data_loader import get_transform, get_loader\n",
    "from imcaption.model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "\n",
    "batch_size = 16             # batch size\n",
    "vocab_threshold = 20        # minimum word count threshold\n",
    "vocab_from_file = False     # if True, load existing vocab file\n",
    "embed_size = 256            # dimensionality of image and word embeddings\n",
    "hidden_size = 512           # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 5              # number of training epochs\n",
    "save_every = 1              # determines frequency of saving model weights\n",
    "print_every = 2000          # determines window for printing average loss\n",
    "learning_rate = 1e-3        # learning rate passed to the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "juvenile-participant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.63s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.71s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 709/414113 [00:00<00:58, 7086.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:50<00:00, 8221.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the Image Transform\n",
    "transform_train = get_transform()\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify the learnable parameters of the model.\n",
    "params = decoder.parameters()\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-dining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1600/25883], Loss: 1.9225, Perplexity: 6.83790\n",
      "Epoch [1/5], Step [3200/25883], Loss: 2.7035, Perplexity: 14.9322\n",
      "Epoch [1/5], Step [4800/25883], Loss: 1.9755, Perplexity: 7.21037\n",
      "Epoch [1/5], Step [6400/25883], Loss: 2.1127, Perplexity: 8.27038\n",
      "Epoch [1/5], Step [8000/25883], Loss: 1.8666, Perplexity: 6.46647\n",
      "Epoch [1/5], Step [9600/25883], Loss: 2.3197, Perplexity: 10.1729\n",
      "Epoch [1/5], Step [11200/25883], Loss: 1.7688, Perplexity: 5.86393\n",
      "Epoch [1/5], Step [12800/25883], Loss: 2.3968, Perplexity: 10.9880\n",
      "Epoch [1/5], Step [14400/25883], Loss: 2.0368, Perplexity: 7.66599\n",
      "Epoch [1/5], Step [16000/25883], Loss: 1.9360, Perplexity: 6.93120\n",
      "Epoch [1/5], Step [17600/25883], Loss: 2.1620, Perplexity: 8.68825\n",
      "Epoch [1/5], Step [19200/25883], Loss: 1.5011, Perplexity: 4.48641\n",
      "Epoch [1/5], Step [20800/25883], Loss: 1.8773, Perplexity: 6.535795\n",
      "Epoch [1/5], Step [22400/25883], Loss: 2.0517, Perplexity: 7.78159\n",
      "Epoch [1/5], Step [24000/25883], Loss: 1.7610, Perplexity: 5.81822\n",
      "Epoch [1/5], Step [25600/25883], Loss: 2.1591, Perplexity: 8.66346\n",
      "Epoch [2/5], Step [1600/25883], Loss: 2.2951, Perplexity: 9.925723\n",
      "Epoch [2/5], Step [3200/25883], Loss: 2.2563, Perplexity: 9.54738\n",
      "Epoch [2/5], Step [4800/25883], Loss: 1.9404, Perplexity: 6.96155\n",
      "Epoch [2/5], Step [6400/25883], Loss: 1.8533, Perplexity: 6.38053\n",
      "Epoch [2/5], Step [8000/25883], Loss: 1.8730, Perplexity: 6.50805\n",
      "Epoch [2/5], Step [9600/25883], Loss: 1.7812, Perplexity: 5.93675\n",
      "Epoch [2/5], Step [11200/25883], Loss: 1.8161, Perplexity: 6.14804\n",
      "Epoch [2/5], Step [12800/25883], Loss: 2.0792, Perplexity: 7.99793\n",
      "Epoch [2/5], Step [14400/25883], Loss: 1.5241, Perplexity: 4.59092\n",
      "Epoch [2/5], Step [16000/25883], Loss: 1.7330, Perplexity: 5.657586\n",
      "Epoch [2/5], Step [17600/25883], Loss: 2.1323, Perplexity: 8.43402\n",
      "Epoch [2/5], Step [19200/25883], Loss: 1.7834, Perplexity: 5.95005\n",
      "Epoch [2/5], Step [20800/25883], Loss: 2.2246, Perplexity: 9.24958\n",
      "Epoch [2/5], Step [22400/25883], Loss: 1.9345, Perplexity: 6.92040\n",
      "Epoch [2/5], Step [24000/25883], Loss: 1.8288, Perplexity: 6.22664\n",
      "Epoch [2/5], Step [24331/25883], Loss: 1.9677, Perplexity: 7.15445"
     ]
    }
   ],
   "source": [
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for i_step in range(1, total_step+1):\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "\n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-default",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_caption",
   "language": "python",
   "name": "conda-env-image_caption-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
